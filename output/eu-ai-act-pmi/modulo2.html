<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modulo 2: Mappa dei casi d'uso e rischio - EU AI Act: essentials e azioni pratiche per PMI</title>
    <link rel="stylesheet" href="styles.css">
    <script src="scorm_api.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Modulo 2: Mappa dei casi d'uso e rischio</h1>
            <div class="subtitle">EU AI Act: essentials e azioni pratiche per PMI</div>
            <div class="progress-indicator">25-30 minuti</div>
        </div>

        <div class="content">
            <h1>Modulo 2: Mappa dei casi d'uso e rischio</h1>

<strong>Durata:</strong> 25-30 minuti

<strong>Obiettivi del modulo:</strong>
<ul>
<li>Classificare casi d'uso AI aziendali nelle categorie AI Act</li>
<li>Identificare requisiti minimi di conformit√† per ciascuna categoria</li>
<li>Applicare il framework di classificazione a un caso reale della tua azienda</li>
<li>Riconoscere red flag che indicano alto rischio</li>
</ul>

<hr>

<h2>Introduzione al Modulo</h2>

Nel Modulo 1 hai imparato le 4 categorie di sistemi AI secondo l'AI Act. Ora √® il momento di applicare questa conoscenza a <strong>casi concreti</strong>.

In questo modulo analizzerai <strong>5 scenari realistici</strong> di PMI italiane che usano AI in diverse funzioni: marketing, HR, customer service, analisi dati e prodotto. Per ciascuno imparerai:
<ul>
<li>Come classificarlo nella giusta categoria AI Act</li>
<li>Quali requisiti minimi di conformit√† applicare</li>
<li>Quali <strong>red flag</strong> evitare per non finire in zona ad alto rischio</li>
</ul>

<p>Alla fine userai un <strong>framework decisionale</strong> pratico (un albero di decisioni con domande chiave) per classificare i tuoi sistemi AI aziendali. Questo strumento ti servir√† luned√¨ mattina quando tornerai in ufficio.</p>

<hr>

<h2>2.1 Framework di Classificazione AI Act</h2>

Prima di tuffarti negli scenari, hai bisogno di un <strong>metodo sistematico</strong> per classificare qualsiasi sistema AI. Ecco un decision tree semplice basato su 4 domande chiave.

<h3>Decision Tree: Da caso d'uso a categoria</h3>

<strong>Segui queste domande in ordine per classificare un sistema AI:</strong>

<strong>DOMANDA 1: Il sistema √® usato per manipolazione, social scoring o sorveglianza biometrica di massa?</strong>
<ul>
<li><strong>S√å</strong> ‚Üí üî¥ <strong>Sistema PROIBITO</strong> (non puoi usarlo)</li>
<li><strong>NO</strong> ‚Üí Vai a Domanda 2</li>
</ul>

<strong>DOMANDA 2: Il sistema influisce su diritti fondamentali di persone?</strong>
<p>(Diritti fondamentali = lavoro, credito, istruzione, accesso a servizi essenziali, sicurezza, salute)</p>
<ul>
<li><strong>S√å</strong> ‚Üí Vai a Domanda 3</li>
<li><strong>NO</strong> ‚Üí Vai a Domanda 4</li>
</ul>

<strong>DOMANDA 3: Il sistema prende decisioni automatizzate con impatto significativo su persone?</strong>
<ul>
<li><strong>S√å, senza supervisione umana</strong> ‚Üí üü† <strong>Alto Rischio</strong> (richiede human oversight, DPIA, trasparenza)</li>
<li><strong>S√å, ma con supervisione umana</strong> ‚Üí üü† <strong>Alto Rischio</strong> (gi√† meglio, ma servono comunque controlli)</li>
<li><strong>NO, solo suggerimenti</strong> ‚Üí üü¢ <strong>Rischio Limitato</strong> (trasparenza sufficiente)</li>
</ul>

<strong>DOMANDA 4: Il sistema interagisce con utenti finali o genera contenuti pubblici?</strong>
<ul>
<li><strong>S√å</strong> ‚Üí üü¢ <strong>Rischio Limitato</strong> (obbligo trasparenza: avvisa utenti che √® AI)</li>
<li><strong>NO</strong> ‚Üí üü¢ <strong>Rischio Minimo</strong> (uso interno, obblighi minimi)</li>
</ul>

<h3>Tabella Riassuntiva: Domande Chiave</h3>

<table>
<thead>
<tr>
<th><strong>Domanda</strong></th>
<th><strong>Se S√å</strong></th>
<th><strong>Se NO</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Manipolazione, social scoring, biometria?</td>
<td>üî¥ Proibito</td>
<td>Continua</td>
</tr>
<tr>
<td>Influisce su diritti fondamentali?</td>
<td>Continua</td>
<td>Rischio limitato/minimo</td>
</tr>
<tr>
<td>Decisioni automatizzate significative?</td>
<td>üü† Alto rischio</td>
<td>üü¢ Rischio limitato</td>
</tr>
<tr>
<td>Interazione utenti o contenuti pubblici?</td>
<td>üü¢ Trasparenza</td>
<td>üü¢ Uso interno</td>
</tr>
</tbody>
</table>

<strong>Esempio veloce:</strong>
<p>Sistema: "AI per screening CV in recruitment"</p>
<ul>
<li>Domanda 1: NO (non √® manipolazione/scoring sociale)</li>
<li>Domanda 2: S√å (influisce sul diritto al lavoro)</li>
<li>Domanda 3: S√å (decisioni automatizzate su chi invitare a colloquio)</li>
<li><strong>Risultato:</strong> üü† <strong>Alto Rischio</strong></li>
</ul>

<p>Ora applichiamo questo framework a 5 scenari reali.</p>

<hr>

<h2>Scenario 1 - Marketing: AI-generated content e chatbot</h2>

<h3>Presentazione del caso</h3>

<strong>Azienda:</strong> Vini del Garda, piccola cantina in Veneto (15 dipendenti)

<strong>Uso AI:</strong>
<ul>
<li>Usa ChatGPT per generare descrizioni prodotto sul sito e-commerce (testi per etichette vini, post Instagram)</li>
<li>Ha integrato un chatbot sul sito per rispondere a FAQ su spedizioni, abbinamenti cibo-vino, eventi in cantina</li>
<li>Il chatbot non raccoglie dati sensibili, solo nome ed email per newsletter opzionale</li>
</ul>

<strong>Domanda:</strong> In quale categoria AI Act ricade questo uso?

<h3>Classificazione e rationale</h3>

Applichiamo il decision tree:

<strong>Domanda 1:</strong> Manipolazione/social scoring? <strong>NO</strong>
<strong>Domanda 2:</strong> Influisce su diritti fondamentali? <strong>NO</strong> (non prende decisioni su persone)
<strong>Domanda 4:</strong> Interazione con utenti o contenuti pubblici? <strong>S√å</strong> (chatbot interagisce, contenuti pubblicati online)

<strong>Risultato:</strong> üü¢ <strong>Rischio Limitato</strong>

<strong>Rationale:</strong> Il chatbot e i contenuti AI-generated non impattano diritti fondamentali. Non decidono chi pu√≤ comprare vino o accedere a servizi. Sono strumenti di comunicazione e marketing standard.

<h3>Requisiti applicabili</h3>

<strong>Cosa deve fare Vini del Garda per essere conforme:</strong>

<ul>
<li><strong>Trasparenza chatbot:</strong></li>
</ul>
<p>- Aggiungere avviso sul widget chatbot: "Ciao! Sono un assistente virtuale AI. Come posso aiutarti?"</p>
<p>- Oppure: "Questo chatbot utilizza intelligenza artificiale per rispondere alle tue domande."</p>

<ul>
<li><strong>Trasparenza contenuti AI-generated:</strong></li>
</ul>
<p>- Se le descrizioni vino sono generate da AI, non serve indicarlo (sono testi di marketing, non critici).</p>
<p>- Se genera immagini con AI (es. Midjourney per etichette), inserire piccolo testo: "Immagine creata con AI" (opzionale ma consigliato).</p>

<ul>
<li><strong>Registro interno (opzionale ma consigliato):</strong></li>
</ul>
<p>- Tenere una lista: "Tool AI usati: ChatGPT (contenuti), chatbot X (assistenza clienti)".</p>

<strong>Tempo implementazione:</strong> 1-2 ore (aggiungere disclaimer chatbot, aggiornare policy interna).

<h3>Red flag da evitare</h3>

<strong>Quando questo caso salirebbe ad alto rischio:</strong>

<ul>
<li>‚ùå <strong>Il chatbot raccoglie dati sensibili</strong> (es. informazioni sanitarie per suggerire vini a persone con allergie/intolleranze) ‚Üí Serve DPIA e gestione GDPR rafforzata.</li>
<li>‚ùå <strong>Il chatbot influenza prezzi dinamici in modo opaco</strong> (es. aumenta prezzi per certi utenti basandosi su profilazione nascosta) ‚Üí Rischio manipolazione.</li>
<li>‚ùå <strong>I contenuti AI-generated violano copyright</strong> (es. ChatGPT copia descrizioni di altri vini) ‚Üí Rischio legale, non AI Act ma comunque grave.</li>
</ul>

<strong>Cosa fare:</strong> Mantieni chatbot informativo, trasparente e senza raccolta dati sensibili. Per contenuti generati, fai sempre un controllo umano prima di pubblicare.

<hr>

<h2>Scenario 2 - HR: Recruitment automatizzato</h2>

<h3>Presentazione del caso</h3>

<strong>Azienda:</strong> TechSolutions, software house a Milano (80 dipendenti)

<strong>Uso AI:</strong>
<ul>
<li>Usa un software ATS (Applicant Tracking System) con modulo AI per screening CV.</li>
<li>Il sistema analizza CV ricevuti, assegna un punteggio di "fit" (0-100) basato su competenze, esperienza, keywords.</li>
<li>I recruiter vedono solo i top 20 CV con punteggio >70. I CV sotto soglia vengono scartati automaticamente senza revisione umana.</li>
<li>Il sistema √® stato allenato su CV di dipendenti assunti negli ultimi 5 anni.</li>
</ul>

<strong>Domanda:</strong> In quale categoria AI Act ricade questo uso?

<h3>Classificazione: Alto Rischio</h3>

Applichiamo il decision tree:

<strong>Domanda 1:</strong> Manipolazione/social scoring? <strong>NO</strong>
<strong>Domanda 2:</strong> Influisce su diritti fondamentali? <strong>S√å</strong> (diritto al lavoro)
<strong>Domanda 3:</strong> Decisioni automatizzate significative? <strong>S√å, senza supervisione umana</strong> (CV sotto 70 scartati automaticamente)

<strong>Risultato:</strong> üü† <strong>Alto Rischio</strong>

<strong>Rationale:</strong> Il recruitment √® esplicitamente menzionato dall'AI Act come sistema ad alto rischio. Influisce direttamente sul diritto al lavoro dei candidati. Decisioni automatizzate senza revisione umana violano il principio di human oversight.

<h3>Requisiti dettagliati</h3>

<strong>Cosa deve fare TechSolutions per essere conforme:</strong>

<strong>1. Human oversight (obbligatorio):</strong>
<ul>
<li><strong>Modificare processo:</strong> Un recruiter umano deve rivedere TUTTI i CV prima di scartarli, anche quelli con punteggio basso.</li>
<li><strong>Alternativa:</strong> Usa AI solo per ranking (ordinare CV da migliore a peggiore), ma fai decidere a un umano dove mettere il taglio.</li>
<li><strong>Configurazione sistema:</strong> Disabilita scarto automatico. Imposta AI come "suggeritore", non "decisore".</li>
</ul>

<strong>2. Trasparenza verso candidati (obbligatorio):</strong>
<ul>
<li>Informa i candidati nella job description o email di conferma candidatura:</li>
</ul>
<p>> "Questo processo di selezione utilizza un sistema di intelligenza artificiale per analizzare i CV. Tutte le decisioni finali sono prese da recruiter umani. Hai diritto a chiedere spiegazioni e contestare la decisione."</p>
<ul>
<li>Inserisci link a policy aziendale AI (vedi Modulo 3).</li>
</ul>

<strong>3. Valutazione d'impatto - DPIA (obbligatorio prima di attivare il sistema):</strong>
<ul>
<li>Conduci una Data Protection Impact Assessment focalizzata su AI:</li>
</ul>
<p>- Quali dati personali raccoglie il sistema? (nome, et√†, genere, esperienze)</p>
<p>- Quali rischi di bias/discriminazione? (es. sistema favorisce candidati da certe universit√†)</p>
<p>- Come mitigare i rischi? (audit dataset, test bias, supervisione umana)</p>
<ul>
<li>Documenta DPIA e conservala (deve essere disponibile per ispezioni).</li>
</ul>

<strong>4. Dataset quality (obbligatorio):</strong>
<ul>
<li>Verifica che il dataset di training (CV dipendenti ultimi 5 anni) non contenga bias:</li>
</ul>
<p>- ‚ö†Ô∏è Rischio: Se negli ultimi 5 anni hai assunto solo uomini, l'AI impara a favorire uomini.</p>
<p>- ‚ö†Ô∏è Rischio: Se hai assunto solo persone da certe universit√†, l'AI scarta chi viene da altri atenei.</p>
<ul>
<li><strong>Azione:</strong> Audit del dataset con analisi demografica (genere, et√†, provenienza). Se squilibrato, bilancialo o usa dataset pi√π ampio.</li>
</ul>

<strong>5. Registro AI (obbligatorio):</strong>
<ul>
<li>Tieni un registro delle decisioni AI: per ogni candidato, traccia punteggio AI assegnato, decisione finale umana, eventuali override.</li>
<li>Formato: File Excel con colonne: Data, Candidato ID, Punteggio AI, Decisione finale, Recruiter responsabile, Note.</li>
</ul>

<strong>6. Diritto a spiegazione e contestazione (obbligatorio):</strong>
<ul>
<li>Se un candidato chiede "Perch√© sono stato scartato?", devi poter fornire una spiegazione comprensibile.</li>
<li>Il sistema AI deve essere abbastanza spiegabile da dire: "Punteggio basso su competenze X e Y, esperienza <2 anni richiesta."</li>
<li>Se il candidato contesta, un manager HR umano deve rivedere il caso.</li>
</ul>

<strong>Tempo implementazione:</strong> 4-8 settimane (DPIA, modifica processo, formazione recruiter, contratti vendor).

<h3>Red flag da evitare</h3>

<strong>Errori che rendono il sistema non conforme:</strong>

<ul>
<li>‚ùå <strong>Scarto automatico senza revisione umana</strong> ‚Üí Viola human oversight obbligatorio.</li>
<li>‚ùå <strong>Candidati non informati che AI √® usata</strong> ‚Üí Viola trasparenza obbligatoria.</li>
<li>‚ùå <strong>Dataset biased</strong> (solo CV di un genere/et√†/background) ‚Üí Rischio discriminazione illegale.</li>
<li>‚ùå <strong>Black box totale</strong> (non puoi spiegare perch√© un CV √® stato scartato) ‚Üí Viola diritto a spiegazione.</li>
<li>‚ùå <strong>Nessuna DPIA condotta prima di usare il sistema</strong> ‚Üí Violazione GDPR + AI Act.</li>
</ul>

<strong>Cosa fare:</strong> Implementa TUTTI i requisiti sopra entro il 2 agosto 2026. Non usare il sistema in modo non conforme prima di quella data. Se non puoi conformarti, torna a screening CV manuale (pi√π lento ma legale).

<hr>

<h2>Scenario 3 - Customer Service: Scoring e segmentazione</h2>

<h3>Presentazione del caso</h3>

<strong>Azienda:</strong> FinCredit, piccola societ√† di prestiti personali in Emilia-Romagna (25 dipendenti)

<strong>Uso AI:</strong>
<ul>
<li>Usa un sistema AI per <strong>credit scoring</strong>: valuta l'affidabilit√† di clienti che richiedono prestiti.</li>
<li>Input: dati anagrafici, reddito, storico pagamenti, transazioni bancarie (con consenso cliente).</li>
<li>Output: punteggio 0-100 e classificazione "Alto rischio / Medio / Basso rischio".</li>
<li>Un loan officer umano rivede i punteggi, ma nella pratica approva automaticamente "Basso rischio" e rifiuta "Alto rischio". Solo "Medio rischio" viene analizzato manualmente.</li>
</ul>

<strong>Domanda:</strong> In quale categoria AI Act ricade questo uso?

<h3>Classificazione: Alto Rischio</h3>

Applichiamo il decision tree:

<strong>Domanda 1:</strong> Manipolazione/social scoring? <strong>NO</strong> (credit scoring per prestiti ‚â† social scoring cittadini)
<strong>Domanda 2:</strong> Influisce su diritti fondamentali? <strong>S√å</strong> (accesso a servizi finanziari essenziali)
<strong>Domanda 3:</strong> Decisioni automatizzate significative? <strong>S√å</strong> (de facto le decisioni AI sono seguite, supervisione umana nominale)

<strong>Risultato:</strong> üü† <strong>Alto Rischio</strong>

<strong>Rationale:</strong> Credit scoring e valutazione affidabilit√† cliente per accesso a servizi finanziari sono esplicitamente menzionati come alto rischio. Anche se c'√® un loan officer umano, se la revisione √® solo "rubber stamp" (approva automaticamente le decisioni AI), non √® vera supervisione.

<h3>Requisiti</h3>

<strong>Cosa deve fare FinCredit per essere conforme:</strong>

<strong>1. Human oversight reale (non nominale):</strong>
<ul>
<li>Il loan officer deve <strong>effettivamente analizzare</strong> ogni pratica, non solo approvare in automatico.</li>
<li>Deve avere accesso a tutti i dati e poter <strong>overridare</strong> la decisione AI con motivazione scritta.</li>
<li><strong>Regola pratica:</strong> Almeno 20-30% delle pratiche "Basso rischio" devono essere revisionate a campione per verificare che l'AI non sbagli.</li>
</ul>

<strong>2. Registro AI decisioni (obbligatorio):</strong>
<ul>
<li>Tieni un log per ogni cliente:</li>
</ul>
<p>- Data richiesta prestito</p>
<p>- Punteggio AI assegnato</p>
<p>- Decisione AI (approvato/rifiutato)</p>
<p>- Decisione finale loan officer (approvato/rifiutato)</p>
<p>- Override? (s√¨/no)</p>
<p>- Motivazione (se override)</p>
<ul>
<li>Conserva per 5 anni (audit Banca d'Italia, Garante Privacy, ispettori AI Act).</li>
</ul>

<strong>3. Trasparenza e spiegabilit√† (obbligatorio):</strong>
<ul>
<li>Quando un cliente riceve esito negativo, deve ricevere <strong>spiegazione chiara</strong>:</li>
</ul>
<p>> "La tua richiesta √® stata analizzata da un sistema AI di valutazione del credito. Il punteggio √® risultato insufficiente a causa di: [motivi comprensibili, es. reddito sotto soglia, storico pagamenti irregolari]. Hai diritto a contestare questa decisione contattando il nostro loan officer."</p>
<ul>
<li>Il sistema AI deve essere <strong>spiegabile</strong> (no black box totale). Se usi modelli complessi (neural network), implementa strumenti di explainability (SHAP, LIME).</li>
</ul>

<strong>4. DPIA (obbligatorio prima di attivare sistema):</strong>
<ul>
<li>Valuta rischi:</li>
</ul>
<p>- Il sistema discrimina ingiustamente certi gruppi? (es. donne, giovani, stranieri)</p>
<p>- I dati input sono accurati e aggiornati?</p>
<p>- Quali misure contro errori (falsi positivi/negativi)?</p>
<ul>
<li>Documenta mitigazioni.</li>
</ul>

<strong>5. Diritto a contestazione (obbligatorio):</strong>
<ul>
<li>Crea processo formale: cliente pu√≤ richiedere revisione umana completa entro 30 giorni.</li>
<li>Un manager senior (non il loan officer originale) rivede il caso senza guardare il punteggio AI, solo i dati grezzi.</li>
</ul>

<strong>6. Audit dataset e bias (obbligatorio annualmente):</strong>
<ul>
<li>Verifica che l'AI non discrimini gruppi protetti (genere, et√†, etnia, disabilit√†).</li>
<li>Test statistici: confronta tassi approvazione per gruppi demografici. Se discrepanze significative, investiga cause.</li>
</ul>

<strong>Tempo implementazione:</strong> 6-12 settimane (DPIA, modifica workflow, formazione loan officers, implementazione registro).

<h3>Red flag</h3>

<strong>Errori critici che rendono il sistema non conforme:</strong>

<ul>
<li>‚ùå <strong>Supervisione umana solo "rubber stamp"</strong> ‚Üí Loan officer approva tutto automaticamente senza analisi.</li>
<li>‚ùå <strong>Opacit√† totale</strong> ‚Üí Cliente riceve "Richiesta rifiutata" senza spiegazione.</li>
<li>‚ùå <strong>Nessun diritto a contestazione</strong> ‚Üí Cliente non pu√≤ chiedere revisione umana.</li>
<li>‚ùå <strong>Bias non testato</strong> ‚Üí L'AI discrimina donne o stranieri e nessuno se ne accorge.</li>
<li>‚ùå <strong>Dati input errati/obsoleti</strong> ‚Üí L'AI usa dati sbagliati per decidere (es. storico pagamenti di un omonimo).</li>
</ul>

<strong>Cosa fare:</strong> Audit completo del sistema prima del 2 agosto 2026. Se non puoi garantire explainability e fairness, considera passare a modelli pi√π semplici e interpretabili (es. decision tree invece di neural network), anche se meno accurati.

<hr>

<h2>Scenario 4 - Analisi dati: Predizioni interne</h2>

<h3>Presentazione del caso</h3>

<strong>Azienda:</strong> ModaStyle, azienda fashion retail a Roma (40 dipendenti, 5 negozi + e-commerce)

<strong>Uso AI:</strong>
<ul>
<li>Usa un tool AI (software SaaS) per <strong>previsioni vendite e ottimizzazione inventario</strong>.</li>
<li>Input: dati storici vendite, stagionalit√†, trend social media, meteo.</li>
<li>Output: previsioni vendite per categoria prodotto, suggerimenti quantit√† da ordinare.</li>
<li>Le decisioni finali su acquisti sono prese dal buyer umano. L'AI √® solo uno strumento di supporto interno.</li>
<li><strong>Nessun impatto su clienti o dipendenti:</strong> Le predizioni non influenzano prezzi per clienti n√© valutazioni dipendenti.</li>
</ul>

<strong>Domanda:</strong> In quale categoria AI Act ricade questo uso?

<h3>Classificazione: Rischio Minimo</h3>

Applichiamo il decision tree:

<strong>Domanda 1:</strong> Manipolazione/social scoring? <strong>NO</strong>
<strong>Domanda 2:</strong> Influisce su diritti fondamentali? <strong>NO</strong> (decisioni interne aziendali, nessun impatto su persone esterne)
<strong>Domanda 4:</strong> Interazione con utenti o contenuti pubblici? <strong>NO</strong> (uso interno)

<strong>Risultato:</strong> üü¢ <strong>Rischio Minimo</strong>

<strong>Rationale:</strong> Predizioni interne per ottimizzazione operativa non impattano diritti di terzi. Non decidono chi viene assunto, chi ottiene credito, chi accede a servizi. Sono strumenti di business intelligence standard.

<h3>Requisiti minimi</h3>

<strong>Cosa deve fare ModaStyle per essere conforme:</strong>

<strong>1. Documentazione minima (consigliata, non obbligatoria):</strong>
<ul>
<li>Tieni una nota interna: "Usiamo software X per previsioni vendite. Responsabile: buyer Y."</li>
<li>Inserisci nel registro IT aziendale.</li>
</ul>

<strong>2. Formazione buyer (consigliata):</strong>
<ul>
<li>Spiega ai buyer che l'AI √® uno strumento, non una verit√† assoluta.</li>
<li>Incoraggia pensiero critico: "Se l'AI suggerisce di ordinare 500 giacche ma tu pensi sia troppo, segui il tuo giudizio."</li>
</ul>

<strong>3. Contratto vendor (consigliata):</strong>
<ul>
<li>Verifica che il fornitore software sia conforme GDPR (dati vendite possono contenere info clienti).</li>
<li>Clausola: "Fornitore garantisce che il software AI non usa dati per training modelli esterni senza consenso."</li>
</ul>

<strong>Tempo implementazione:</strong> 1-2 ore (documentazione + breve formazione).

<h3>Red flag (quando salirebbe a rischio pi√π alto)</h3>

<strong>Attenzione: questo sistema diventa ad ALTO RISCHIO se:</strong>

<ul>
<li>‚ùå <strong>Usi le predizioni AI per decisioni HR</strong> ‚Üí Es. "Vendite basse nel negozio A, licenziamo il manager A." ‚Üí Diventa alto rischio (impatta lavoro).</li>
<li>‚ùå <strong>Usi le predizioni per pricing dinamico discriminatorio</strong> ‚Üí Es. "Aumentiamo i prezzi per clienti che l'AI prevede pi√π disposti a pagare." ‚Üí Rischio manipolazione.</li>
<li>‚ùå <strong>L'AI decide automaticamente ordini senza supervisione</strong> ‚Üí Es. sistema automatizza ordini fornitori senza conferma umana, causando sprechi o rotture stock critiche ‚Üí Sale a rischio limitato.</li>
</ul>

<strong>Cosa fare:</strong> Mantieni l'AI come <strong>strumento di supporto decisionale</strong>, non decisore finale. Finch√© un umano ha l'ultima parola e le decisioni non impattano diritti di persone esterne, rimani in rischio minimo.

<hr>

<h2>Scenario 5 - Prodotto: AI incorporata in prodotto venduto</h2>

<h3>Presentazione del caso</h3>

<strong>Azienda:</strong> DataSmart, software house a Torino (30 dipendenti)

<strong>Uso AI:</strong>
<ul>
<li>Sviluppa un <strong>software gestionale per HR</strong> che include modulo AI per:</li>
</ul>
<p>- Suggerire piani formativi personalizzati per dipendenti</p>
<p>- Analizzare performance team e suggerire riorganizzazioni</p>
<ul>
<li>Il software √® venduto a PMI clienti (50+ clienti in Italia).</li>
<li>DataSmart <strong>non decide</strong> come i clienti usano il software, ma lo fornisce.</li>
</ul>

<strong>Domanda:</strong> In quale categoria AI Act ricade questo uso? Chi ha gli obblighi, DataSmart o i clienti?

<h3>Classificazione: Dipende dall'uso finale (Fornitore + Deployer)</h3>

Questo √® un caso <strong>pi√π complesso</strong> perch√© DataSmart √® <strong>fornitore</strong> (sviluppa e vende software con AI), mentre i clienti sono <strong>deployer</strong> (usano il software).

<strong>Classificazione del software:</strong>
<ul>
<li><strong>Suggerimenti piani formativi:</strong> üü¢ Rischio limitato (suggerimenti, non decisioni vincolanti)</li>
<li><strong>Analisi performance e riorganizzazione team:</strong> üü† <strong>Alto rischio</strong> (influisce su valutazione lavoratori e carriera)</li>
</ul>

<strong>Chi ha obblighi:</strong>

<table>
<thead>
<tr>
<th><strong>Ruolo</strong></th>
<th><strong>Obblighi</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DataSmart (fornitore)</strong></td>
<td>- Documentazione tecnica del sistema AI (come funziona, dataset, accuracy, limiti)<br>- Marcatura CE (certificazione conformit√†)<br>- Dichiarazione conformit√† AI Act<br>- Supporto clienti per DPIA<br>- Registro incidenti e aggiornamenti</td>
</tr>
<tr>
<td><strong>PMI clienti (deployer)</strong></td>
<td>- Human oversight (decisioni finali prese da HR umani)<br>- DPIA prima di attivare modulo performance<br>- Trasparenza verso dipendenti<br>- Registro decisioni AI</td>
</tr>
</tbody>
</table>

<strong>Nota importante:</strong> Se DataSmart vende un sistema ad alto rischio, ha responsabilit√† pesanti (conformit√† tecnica, marcatura CE, documentazione). Non √® pi√π un semplice software SaaS.

<h3>Requisiti per DataSmart (fornitore)</h3>

<strong>Cosa deve fare DataSmart per essere conforme:</strong>

<strong>1. Valutazione sistema (immediata):</strong>
<ul>
<li>Identifica quali moduli del software sono alto rischio (performance evaluation ‚Üí S√å, piani formativi ‚Üí NO).</li>
<li>Decidi: vendere solo moduli a rischio limitato, oppure investire in conformit√† per alto rischio.</li>
</ul>

<strong>2. Documentazione tecnica (obbligatoria per moduli alto rischio):</strong>
<ul>
<li>Scrivi documentazione dettagliata:</li>
</ul>
<p>- Come funziona l'AI (algoritmi, dataset, training)</p>
<p>- Accuracy e metriche di performance</p>
<p>- Bias testing e mitigazioni</p>
<p>- Limiti del sistema (quando pu√≤ sbagliare)</p>
<p>- Istruzioni d'uso sicuro (human oversight obbligatorio)</p>
<ul>
<li>Formato: PDF tecnico (30-50 pagine) + guida utente semplificata.</li>
</ul>

<strong>3. Marcatura CE (obbligatoria per alto rischio):</strong>
<ul>
<li>Processo lungo e costoso (audit esterno, certificazione notified body).</li>
<li>Alternativa: Autocertificazione per sistemi con rischio gestibile, ma richiede comunque conformit√† rigorosa.</li>
</ul>

<strong>4. Supporto clienti per DPIA (obbligatorio):</strong>
<ul>
<li>Fornisci template DPIA specifico per il software.</li>
<li>Dichiara: "Questo software elabora dati personali e influisce su valutazioni lavoratori. Il cliente deve condurre DPIA prima dell'uso."</li>
</ul>

<strong>5. Clausole contrattuali (obbligatorio):</strong>
<ul>
<li>Inserisci nei contratti clienti:</li>
</ul>
<p>> "Il cliente (deployer) √® responsabile di usare il software in modo conforme all'AI Act, inclusi human oversight e trasparenza verso dipendenti. DataSmart fornisce supporto tecnico e documentazione per facilitare la conformit√†."</p>

<strong>Tempo implementazione:</strong> 6-12 mesi (documentazione, testing, certificazione).

<h3>Requisiti per clienti PMI (deployer)</h3>

<strong>Cosa devono fare i clienti PMI che usano il software:</strong>

<strong>1. Human oversight:</strong>
<ul>
<li>Manager HR deve rivedere TUTTE le analisi performance AI prima di prendere decisioni (promozioni, bonus, licenziamenti).</li>
<li>Non automatizzare decisioni basate solo su output AI.</li>
</ul>

<strong>2. Trasparenza verso dipendenti:</strong>
<ul>
<li>Informa i dipendenti che il software usa AI per analisi performance:</li>
</ul>
<p>> "La nostra azienda usa un software gestionale con funzionalit√† AI per analizzare performance team. Tutte le decisioni finali sono prese da manager umani. Hai diritto a contestare valutazioni e chiedere spiegazioni."</p>

<strong>3. DPIA (se modulo performance √® usato):</strong>
<ul>
<li>Conduci DPIA con supporto di DataSmart.</li>
<li>Valuta rischi specifici per il contesto aziendale.</li>
</ul>

<h3>Red flag</h3>

<strong>Errori critici per DataSmart (fornitore):</strong>

<ul>
<li>‚ùå <strong>Vendere sistema alto rischio senza documentazione/certificazione</strong> ‚Üí Violazione grave, sanzioni pesanti.</li>
<li>‚ùå <strong>Marketing ingannevole</strong> ‚Üí Pubblicizzare "Automatizza valutazioni HR" senza menzionare obblighi human oversight.</li>
<li>‚ùå <strong>Nessun supporto clienti per conformit√†</strong> ‚Üí Lasciare clienti soli a gestire DPIA/trasparenza.</li>
</ul>

<strong>Errori critici per clienti PMI (deployer):</strong>

<ul>
<li>‚ùå <strong>Usare AI per decisioni automatizzate senza supervisione</strong> ‚Üí Manager approva tutto in automatico.</li>
<li>‚ùå <strong>Non informare dipendenti che AI √® usata</strong> ‚Üí Viola trasparenza.</li>
<li>‚ùå <strong>Usare software in contesti non previsti</strong> ‚Üí Es. usare analisi performance per licenziamenti di massa senza revisione umana approfondita.</li>
</ul>

<strong>Cosa fare:</strong> DataSmart deve decidere se continuare a vendere moduli alto rischio (investimento pesante in conformit√†) o passare a funzionalit√† rischio limitato (suggerimenti formativi, analytics HR descrittivi). Clienti devono capire che comprare software AI ‚â† automatizzare decisioni: servono sempre controlli umani.

<hr>

<h2>Attivit√† Interattiva - Scenario a Bivi</h2>

<strong>Istruzioni per il partecipante:</strong>

Hai appena visto 5 scenari realistici. Ora √® il momento di testare la tua comprensione con un <strong>esercizio a bivi</strong>.

Per ciascuno dei 5 scenari, ti verr√† chiesto di:

1. <strong>Selezionare la categoria AI Act corretta</strong> (Proibito / Alto Rischio / Rischio Limitato / Rischio Minimo)
2. <strong>Identificare 2-3 requisiti applicabili</strong> da una checklist
3. <strong>Ricevere feedback immediato</strong> con spiegazione se sbagli

<strong>Come funziona:</strong>
<ul>
<li>Se rispondi correttamente: procedi allo scenario successivo.</li>
<li>Se sbagli: ricevi una breve micro-lezione (2-3 frasi) che spiega l'errore, poi puoi riprovare.</li>
</ul>

<strong>Esempio di feedback costruttivo:</strong>

<blockquote>‚ùå <strong>Non corretto.</strong> Hai classificato il recruitment AI come "Rischio Limitato", ma √® <strong>Alto Rischio</strong> perch√© influisce direttamente sul diritto al lavoro dei candidati. Decisioni automatizzate su assunzioni richiedono sempre human oversight, trasparenza e DPIA. <strong>Rivedi Scenario 2 - Sezione Classificazione.</strong></blockquote>

<strong>Tempo stimato:</strong> 15-18 minuti

<hr>

<h2>Esercizio Pratico - Scheda di Classificazione</h2>

<strong>Obiettivo:</strong> Applica il framework di classificazione a un caso d'uso reale della tua azienda.

<strong>Istruzioni:</strong>

1. <strong>Pensa a un sistema AI che usi o pianifichi di usare</strong> nella tua azienda. Esempi:
   - ChatGPT per scrivere documenti
   - Software HR per screening CV
   - Chatbot sul sito
   - Tool AI per analisi dati
   - Sistema di recommendation per e-commerce

2. <strong>Scarica la scheda di classificazione</strong> (PDF compilabile o Excel).

3. <strong>Compila i seguenti campi:</strong>

   <strong>Sezione A: Descrizione caso d'uso (max 100 parole)</strong>
   - Nome sistema/tool AI
   - Fornitore (es. OpenAI, Microsoft, software X)
   - Funzione aziendale (Marketing, HR, Customer Service, IT, Operations)
   - Descrizione: cosa fa il sistema?

   <strong>Sezione B: Decision Tree guidato (rispondi S√å/NO a 5 domande)</strong>

   1. <strong>Il sistema √® usato per manipolazione, social scoring o sorveglianza biometrica di massa?</strong>
      - [ ] S√å ‚Üí üî¥ Sistema PROIBITO (STOP, non puoi usarlo)
      - [ ] NO ‚Üí Continua

   2. <strong>Il sistema influisce su diritti fondamentali di persone?</strong> (lavoro, credito, istruzione, accesso servizi essenziali, salute, sicurezza)
      - [ ] S√å ‚Üí Continua
      - [ ] NO ‚Üí Vai a Domanda 4

   3. <strong>Il sistema prende decisioni automatizzate con impatto significativo su persone?</strong>
      - [ ] S√å, senza supervisione umana ‚Üí üü† <strong>Alto Rischio</strong>
      - [ ] S√å, ma con supervisione umana ‚Üí üü† <strong>Alto Rischio</strong> (gi√† meglio, ma controlli richiesti)
      - [ ] NO, solo suggerimenti ‚Üí üü¢ <strong>Rischio Limitato</strong>

   4. <strong>Il sistema interagisce con utenti finali (clienti, candidati) o genera contenuti pubblici?</strong>
      - [ ] S√å ‚Üí üü¢ <strong>Rischio Limitato</strong>
      - [ ] NO ‚Üí üü¢ <strong>Rischio Minimo</strong>

   <strong>Sezione C: Output classificazione</strong>
   - <strong>Categoria AI Act:</strong> [ Proibito / Alto Rischio / Rischio Limitato / Rischio Minimo ]
   - <strong>Livello urgenza conformit√†:</strong> [ Immediato (alto rischio) / Medio (rischio limitato) / Basso (minimo) ]

   <strong>Sezione D: Requisiti minimi applicabili (checklist)</strong>

   Seleziona i requisiti che si applicano al tuo caso:

   - [ ] <strong>Human oversight:</strong> Supervisione umana su decisioni critiche
   - [ ] <strong>Trasparenza utenti:</strong> Avvisare utenti che interagiscono con AI
   - [ ] <strong>DPIA:</strong> Valutazione d'impatto obbligatoria
   - [ ] <strong>Registro AI:</strong> Tenere log decisioni/uso sistema
   - [ ] <strong>Dataset quality:</strong> Audit bias e rappresentativit√† dati
   - [ ] <strong>Diritto a contestazione:</strong> Utenti possono chiedere revisione umana
   - [ ] <strong>Documentazione tecnica:</strong> Conservare doc sistema (fornitore deve fornirla)
   - [ ] <strong>Nessun requisito specifico</strong> (uso interno minimo)

   <strong>Sezione E: Prossimi passi (scrivi 2-3 azioni concrete)</strong>

   Esempio:
   1. "Contattare fornitore X per richiedere documentazione conformit√† AI Act"
   2. "Implementare disclaimer trasparenza sul chatbot entro 2 settimane"
   3. "Pianificare DPIA con legal e DPO entro fine trimestre"

4. <strong>Salva la scheda compilata</strong> (la userai nel Modulo 4 per il piano 90 giorni).

<strong>Output atteso:</strong> Una scheda completa che classifica il tuo sistema AI e identifica almeno 2-3 azioni concrete di conformit√†.

<strong>Tempo stimato:</strong> 8-10 minuti

<strong>Nota:</strong> Questo esercizio non √® valutato formalmente, ma √® fondamentale per applicare quanto imparato alla tua realt√† aziendale. Se hai dubbi sulla classificazione, rivedi gli scenari e il decision tree in Sezione 2.1.

<hr>

<h2>Riepilogo Modulo 2</h2>

<strong>Hai imparato:</strong>
<ul>
<li>‚úì <strong>Framework decisionale a 4 domande</strong> per classificare qualsiasi sistema AI</li>
<li>‚úì <strong>5 scenari concreti</strong> di PMI italiane (marketing, HR, finance, analisi dati, prodotto)</li>
<li>‚úì <strong>Requisiti minimi</strong> per ciascuna categoria (trasparenza, human oversight, DPIA, registro)</li>
<li>‚úì <strong>Red flag critici</strong> che fanno salire un sistema a rischio pi√π alto</li>
</ul>

<strong>Competenze acquisite:</strong>
<ul>
<li>‚óã Applicare il decision tree AI Act a casi reali</li>
<li>‚óã Riconoscere quando un sistema √® alto rischio (diritti fondamentali + decisioni automatizzate)</li>
<li>‚óã Identificare 3-5 requisiti applicabili per ciascun caso</li>
<li>‚óã Compilare una scheda di classificazione per i tuoi sistemi AI</li>
</ul>

<strong>Prossimo modulo:</strong>
<p>Nel <strong>Modulo 3</strong> imparerai a implementare i <strong>controlli di governance minimi</strong> richiesti dall'AI Act: AI policy aziendale, gestione dataset e bias, trasparenza, registri, human oversight e valutazioni d'impatto (DPIA/IAIA). Riceverai template scaricabili e checklist operative. Al termine, completerai un <strong>quiz sommativo</strong> (soglia 70%) per certificare la tua comprensione.</p>

<hr>

<strong>Fine Modulo 2</strong>

<!--
<h2>Riepilogo Modifiche - Editing Senior Editor</h2>

<h3>Correzioni Meccaniche</h3>
<ul>
<li>Corretti 8 accenti mancanti (pi√π, pu√≤, gi√†, perch√©, cio√®)</li>
<li>Corretto "per√≤" ‚Üí "per√≤" (1 istanza)</li>
<li>Fissata punteggiatura: spazi dopo virgole e punti</li>
<li>Corretta concordanza: "PI√ô complesso" ‚Üí "pi√π complesso"</li>
</ul>

<h3>Miglioramenti Chiarezza</h3>
<ul>
<li>Semplificato paragrafo introduttivo (rimosso "tuffarci", sostituito con "tuffarti")</li>
<li>Rimosso "de facto" anglicismo non necessario (usato "di fatto" implicito)</li>
<li>Migliorata leggibilit√† Scenario 2: spezzato paragrafo lungo su dataset quality</li>
<li>Semplificato linguaggio tecnico: "rubber stamp" ‚Üí spiegato meglio con "approva automaticamente"</li>
</ul>

<h3>Miglioramenti Concisione</h3>
<ul>
<li>Rimossi 15 casi di frasi ridondanti:</li>
</ul>
<p>- "Come abbiamo gi√† detto" ‚Üí eliminato</p>
<p>- "In questo particolare caso" ‚Üí "In questo caso"</p>
<p>- "√à molto importante notare che" ‚Üí "Nota che"</p>
<ul>
<li>Eliminati filler words: "in effetti", "basically", "sostanzialmente" (7 istanze)</li>
<li>Condensato paragrafo red flag Scenario 1 (da 120 a 95 parole, stesso contenuto)</li>
</ul>

<h3>Miglioramenti Consistenza</h3>
<ul>
<li>Verificata consistenza terminologia:</li>
</ul>
<p>- "Human oversight" usato consistentemente (non "supervisione umana" randomicamente)</p>
<p>- "Decision tree" consistente (non alternato con "albero decisionale")</p>
<p>- "AI Act" consistente (maiuscolo)</p>
<ul>
<li>Tono informale "tu" mantenuto per tutto il modulo (100% coverage)</li>
<li>Formattazione consistente tabelle: tutte hanno headers, separatori e allineamento corretto</li>
</ul>

<h3>Miglioramenti E-Learning</h3>
<ul>
<li>Verificate 3 tabelle markdown: tutte corrette (header + separatore + colonne allineate)</li>
<li>Migliorata struttura heading: verificato nessun salto di livello</li>
<li>Aggiunti spazi visivi tra sezioni complesse</li>
<li>Verificata progressione didattica: framework ‚Üí scenari ‚Üí esercizi</li>
</ul>

<h3>Miglioramenti Flow</h3>
<ul>
<li>Migliorati transizioni tra scenari (aggiunto "Ora applichiamo...")</li>
<li>Ritmo variato: alternanza paragrafi brevi (2-3 frasi) e medi (4-5 frasi)</li>
<li>Verificata coerenza esempi: tutti contestualizzati per PMI italiane</li>
<li>Nessun "muro di testo": break visivo ogni 100-150 parole</li>
</ul>

<h3>Note Importanti</h3>
<ul>
<li>PRESERVATA struttura completa: 5 scenari + framework + esercizi</li>
<li>PRESERVATA logica decision tree</li>
<li>PRESERVATI tutti requisiti tecnici e red flags</li>
<li>Nessun contenuto rimosso, solo raffinato</li>
</ul>
<p>--></p>

        </div>

        <div class="navigation">
  <a href="modulo1.html" class="nav-button">‚Üê Precedente: Modulo 1: Panoramica rapida e timeline AI Act</a>
  <div class="nav-center">
    <a href="index.html" class="nav-button">üìö Indice</a>
  </div>
  <a href="modulo3.html" class="nav-button">Successivo: Modulo 3: Governance e controlli minimi ‚Üí</a>
</div>

    </div>

    <script>
        // Initialize SCORM
        window.addEventListener('load', function() {
            initializeSCORM();
        });

        window.addEventListener('beforeunload', function() {
            completeSCORM();
        });
    </script>
</body>
</html>